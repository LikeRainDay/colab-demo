{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "社恐分析.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNN4tbzlmt2tjfSAKUecQOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LikeRainDay/colab-demo/blob/main/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/%E7%A4%BE%E6%81%90%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWGrd4iL5TJL"
      },
      "source": [
        "# 介绍\n",
        "\n",
        "本章内容主要介绍如何使用sellterGraph进行图相关的算法使用\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW2Uw39_4yEk"
      },
      "source": [
        "# 准备 恐怖袭击的数据集\n",
        "!wget https://gtd.terrorismdata.com/app/uploads/_mediavault/2019/09/globalterrorismdb_0919dist.xlsx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Z9_by250zZ"
      },
      "source": [
        "#准备所需要的依赖包导入"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqDIVNNs54Xz"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  %pip install -q stellargraph[demos]==1.2.1\n",
        "\n",
        "import stellargraph as sg\n",
        "\n",
        "try:\n",
        "    sg.utils.validate_notebook_version(\"1.2.1\")\n",
        "except AttributeError:\n",
        "    raise ValueError(\n",
        "        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
        "    ) from None\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph.data import EdgeSplitter\n",
        "from stellargraph.mapper import GraphSAGELinkGenerator, GraphSAGENodeGenerator\n",
        "from stellargraph.layer import GraphSAGE, link_classification\n",
        "from stellargraph.data import UniformRandomWalk\n",
        "from stellargraph.data import UnsupervisedSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from stellargraph import globalvar\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing, feature_extraction, model_selection\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSQ2VgLJ6Qb8"
      },
      "source": [
        "读取数据集\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwGbNXMn6XA6"
      },
      "source": [
        "dt_raw = pd.read_excel(\n",
        "    \"/content/globalterrorismdb_0919dist.xlsx\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Hp6ai764Hr"
      },
      "source": [
        "dt_raw.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sotuqUTg6bpZ"
      },
      "source": [
        "导入特征文件内容"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE5MDF7L6k3o"
      },
      "source": [
        "from functools import reduce\n",
        "def load_features(input_data):\n",
        "    # Summarise features by terrorist group\n",
        "    dt_collect = input_data[\n",
        "        [\"eventid\", \"nperps\", \"success\", \"suicide\", \"nkill\", \"nwound\", \"gname\"]\n",
        "    ]\n",
        "    dt_collect.fillna(0, inplace=True)\n",
        "    dt_collect.nperps[dt_collect.nperps < 0] = 0\n",
        "\n",
        "    summarize_by_gname = (\n",
        "        dt_collect.groupby(\"gname\")\n",
        "        .agg(\n",
        "            {\n",
        "                \"eventid\": \"count\",\n",
        "                \"nperps\": \"sum\",\n",
        "                \"nkill\": \"sum\",\n",
        "                \"nwound\": \"sum\",\n",
        "                \"success\": \"sum\",\n",
        "            }\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    summarize_by_gname.columns = [\n",
        "        \"gname\",\n",
        "        \"n_attacks\",\n",
        "        \"n_nperp\",\n",
        "        \"n_nkil\",\n",
        "        \"n_nwound\",\n",
        "        \"n_success\",\n",
        "    ]\n",
        "    summarize_by_gname[\"success_ratio\"] = (\n",
        "        summarize_by_gname[\"n_success\"] / summarize_by_gname[\"n_attacks\"]\n",
        "    )\n",
        "    summarize_by_gname.drop([\"n_success\"], axis=1, inplace=True)\n",
        "\n",
        "    # Collect counts of each attack type\n",
        "    dt_collect = input_data[[\"gname\", \"attacktype1_txt\"]]\n",
        "    gname_attacktypes = (\n",
        "        dt_collect.groupby([\"gname\", \"attacktype1_txt\"])[\"attacktype1_txt\"]\n",
        "        .count()\n",
        "        .to_frame()\n",
        "    )\n",
        "    gname_attacktypes.columns = [\"attacktype_count\"]\n",
        "    gname_attacktypes.reset_index(inplace=True)\n",
        "    gname_attacktypes_wide = gname_attacktypes.pivot(\n",
        "        index=\"gname\", columns=\"attacktype1_txt\", values=\"attacktype_count\"\n",
        "    )\n",
        "    gname_attacktypes_wide.fillna(0, inplace=True)\n",
        "    gname_attacktypes_wide.drop([\"Unknown\"], axis=1, inplace=True)\n",
        "\n",
        "    # Collect counts of each target type\n",
        "    dt_collect = input_data[[\"gname\", \"targtype1_txt\"]]\n",
        "    gname_targtypes = (\n",
        "        dt_collect.groupby([\"gname\", \"targtype1_txt\"])[\"targtype1_txt\"]\n",
        "        .count()\n",
        "        .to_frame()\n",
        "    )\n",
        "    gname_targtypes.columns = [\"targtype_count\"]\n",
        "    gname_targtypes.reset_index(inplace=True)\n",
        "    gname_targtypes_wide = gname_targtypes.pivot(\n",
        "        index=\"gname\", columns=\"targtype1_txt\", values=\"targtype_count\"\n",
        "    )\n",
        "    gname_targtypes_wide.fillna(0, inplace=True)\n",
        "    gname_targtypes_wide.drop([\"Unknown\"], axis=1, inplace=True)\n",
        "\n",
        "    # Combine all features\n",
        "    data_frames = [summarize_by_gname, gname_attacktypes_wide, gname_targtypes_wide]\n",
        "    gnames_features = reduce(\n",
        "        lambda left, right: pd.merge(left, right, on=[\"gname\"], how=\"outer\"),\n",
        "        data_frames,\n",
        "    )\n",
        "    return gnames_features\n",
        "\n",
        "\n",
        "gnames_features = load_features(input_data=dt_raw)\n",
        "gnames_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2paF3_177ZWY"
      },
      "source": [
        "# 将输入导入到selletGraph要求的数据格式内容\n",
        "\n",
        "def load_network(input_data):\n",
        "    # Create country_decade feature\n",
        "    dt_collect = input_data[[\"eventid\", \"country_txt\", \"iyear\", \"gname\"]]\n",
        "    dt_collect[\"decade\"] = (dt_collect[\"iyear\"] // 10) * 10\n",
        "    dt_collect[\"country_decade\"] = (\n",
        "        dt_collect[\"country_txt\"] + \"_\" + dt_collect[\"decade\"].map(str) + \"s\"\n",
        "    )\n",
        "    dt_collect = dt_collect[dt_collect.gname != \"Unknown\"]\n",
        "\n",
        "    # Create a country_decade edgelist\n",
        "    gnames_country_decade = (\n",
        "        dt_collect.groupby([\"gname\", \"country_decade\"])\n",
        "        .agg({\"eventid\": \"count\"})\n",
        "        .reset_index()\n",
        "    )\n",
        "    gnames_country_decade_edgelist = pd.merge(\n",
        "        gnames_country_decade, gnames_country_decade, on=\"country_decade\", how=\"left\"\n",
        "    )\n",
        "    gnames_country_decade_edgelist.drop(\n",
        "        [\"eventid_x\", \"eventid_y\"], axis=1, inplace=True\n",
        "    )\n",
        "    gnames_country_decade_edgelist.columns = [\"source\", \"country_decade\", \"target\"]\n",
        "    gnames_country_decade_edgelist = gnames_country_decade_edgelist[\n",
        "        gnames_country_decade_edgelist.source != gnames_country_decade_edgelist.target\n",
        "    ]\n",
        "\n",
        "    G_country_decade = nx.from_pandas_edgelist(\n",
        "        gnames_country_decade_edgelist, source=\"source\", target=\"target\"\n",
        "    )\n",
        "\n",
        "    # Create edgelist from the related column\n",
        "    dt_collect = input_data[\"related\"]\n",
        "    dt_collect.dropna(inplace=True)\n",
        "    gname_event_mapping = input_data[[\"eventid\", \"gname\"]].drop_duplicates()\n",
        "    gname_event_mapping.eventid = gname_event_mapping.eventid.astype(str)\n",
        "\n",
        "    G_related = nx.parse_adjlist(\n",
        "        dt_collect.values, delimiter=\", \"\n",
        "    )  # attacks that are related\n",
        "    df_related = nx.to_pandas_edgelist(G_related)\n",
        "    df_related.replace(\" \", \"\", regex=True, inplace=True)\n",
        "    df_source_gname = pd.merge(\n",
        "        df_related,\n",
        "        gname_event_mapping,\n",
        "        how=\"left\",\n",
        "        left_on=\"source\",\n",
        "        right_on=\"eventid\",\n",
        "    )\n",
        "    df_source_gname.rename(columns={\"gname\": \"gname_source\"}, inplace=True)\n",
        "    df_target_gname = pd.merge(\n",
        "        df_source_gname,\n",
        "        gname_event_mapping,\n",
        "        how=\"left\",\n",
        "        left_on=\"target\",\n",
        "        right_on=\"eventid\",\n",
        "    )\n",
        "    df_target_gname.rename(columns={\"gname\": \"gname_target\"}, inplace=True)\n",
        "\n",
        "    # Filtering and cleaning\n",
        "    gnames_relations_edgelist = df_target_gname[\n",
        "        df_target_gname.gname_source != df_target_gname.gname_target\n",
        "    ]\n",
        "    gnames_relations_edgelist = gnames_relations_edgelist[\n",
        "        gnames_relations_edgelist.gname_source != \"Unknown\"\n",
        "    ]\n",
        "    gnames_relations_edgelist = gnames_relations_edgelist[\n",
        "        gnames_relations_edgelist.gname_target != \"Unknown\"\n",
        "    ]\n",
        "    gnames_relations_edgelist = gnames_relations_edgelist[\n",
        "        [\"gname_source\", \"gname_target\"]\n",
        "    ]\n",
        "    gnames_relations_edgelist.dropna(inplace=True)\n",
        "\n",
        "    G_rel = nx.from_pandas_edgelist(\n",
        "        gnames_relations_edgelist, source=\"gname_source\", target=\"gname_target\"\n",
        "    )\n",
        "\n",
        "    # Merging two graphs\n",
        "    G = nx.compose(G_country_decade, G_rel)\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "\n",
        "G = load_network(input_data=dt_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nsl74bo7p0h"
      },
      "source": [
        "print(nx.info(G))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZSzSX5K7uDa"
      },
      "source": [
        "# 联通数量\n",
        "\n",
        "print(nx.number_connected_components(G))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SE5PGg871GH"
      },
      "source": [
        "subGraph = (G.subgraph(c) for c in nx.connected_components(G))\n",
        "Gcc = sorted(subGraph, key=len, reverse=True)\n",
        "cc_sizes = []\n",
        "for cc in list(Gcc):\n",
        "    cc_sizes.append(len(cc.nodes()))\n",
        "print(cc_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOIBxZ8M8_Ux"
      },
      "source": [
        "filtered_features = gnames_features[gnames_features[\"gname\"].isin(list(G.nodes()))]\n",
        "filtered_features.set_index(\"gname\", inplace=True)\n",
        "filtered_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1T_oau_9Avp"
      },
      "source": [
        "filtered_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boOl2vKa9Jl0"
      },
      "source": [
        "node_features = filtered_features.transform(lambda x: np.log1p(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1ts4IPn9NeS"
      },
      "source": [
        "# 检查是否有拼写错误的内容遗留\n",
        "set(list(G.nodes())) - set(list(node_features.index.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E8jNRfw8siD"
      },
      "source": [
        "# Unsupervised graphSAGE 无监督graphSAGE 的功能实现内容"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKBhe7Iq8Us6"
      },
      "source": [
        "Gs = sg.StellarGraph.from_networkx(G, node_features=node_features)\n",
        "print(Gs.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYAHzoxZ9cpL"
      },
      "source": [
        "# 模型参数\n",
        "number_of_walks = 3\n",
        "length = 5\n",
        "batch_size = 50\n",
        "epochs = 10\n",
        "num_samples = [20, 20]\n",
        "layer_sizes = [100, 100]\n",
        "learning_rate = 1e-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlva21Vs9gE9"
      },
      "source": [
        "# 非监督采样\n",
        "unsupervisedSamples = UnsupervisedSampler(\n",
        "    Gs, nodes=G.nodes(), length=length, number_of_walks=number_of_walks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqtreI_c9nxb"
      },
      "source": [
        "generator = GraphSAGELinkGenerator(Gs, batch_size, num_samples)\n",
        "train_gen = generator.flow(unsupervisedSamples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b_bUAjK9yxB"
      },
      "source": [
        "assert len(layer_sizes) == len(num_samples)\n",
        "\n",
        "graphsage = GraphSAGE(\n",
        "    layer_sizes=layer_sizes, generator=generator, bias=True, dropout=0.0, normalize=\"l2\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJFeHV3U93Ve"
      },
      "source": [
        "# 进行连接预测内容\n",
        "x_inp, x_out = graphsage.in_out_tensors()\n",
        "\n",
        "prediction = link_classification(\n",
        "    output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\"\n",
        ")(x_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTT9n7QF97nL"
      },
      "source": [
        "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
        "\n",
        "model.compile(\n",
        "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "        loss=keras.losses.binary_crossentropy,\n",
        "        metrics=[keras.metrics.binary_accuracy],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qig8B31Z9-W0"
      },
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        "    use_multiprocessing=False,\n",
        "    workers=1,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLn4gg_TC7OO"
      },
      "source": [
        "node_ids = list(Gs.nodes())\n",
        "node_gen = GraphSAGENodeGenerator(Gs, batch_size, num_samples).flow(node_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb0nVmAEC8FI"
      },
      "source": [
        "embedding_model = keras.Model(inputs=x_inp[::2], outputs=x_out[0])\n",
        "node_embeddings = embedding_model.predict(node_gen, workers=4, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AabNQn7cDCcL"
      },
      "source": [
        "# 可视化映射内容\n",
        "node_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKYTsohhDGu5"
      },
      "source": [
        "X = node_embeddings\n",
        "if X.shape[1] > 2:\n",
        "    transform = TSNE  # PCA\n",
        "\n",
        "    trans = transform(n_components=2, random_state=123)\n",
        "    emb_transformed = pd.DataFrame(trans.fit_transform(X), index=node_ids)\n",
        "else:\n",
        "    emb_transformed = pd.DataFrame(X, index=node_ids)\n",
        "    emb_transformed = emb_transformed.rename(columns={\"0\": 0, \"1\": 1})\n",
        "\n",
        "alpha = 0.7\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "ax.scatter(emb_transformed[0], emb_transformed[1], alpha=alpha)\n",
        "ax.set(aspect=\"equal\", xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
        "plt.title(\"{} visualization of GraphSAGE embeddings\".format(transform.__name__))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HovMbS4DMsM"
      },
      "source": [
        "emb_transformed[\"infomap_clusters\"] = emb_transformed.index.map(infomap_com_dict)\n",
        "plt.scatter(\n",
        "    emb_transformed[0],\n",
        "    emb_transformed[1],\n",
        "    c=emb_transformed[\"infomap_clusters\"],\n",
        "    cmap=\"Spectral\",\n",
        "    edgecolors=\"black\",\n",
        "    alpha=0.3,\n",
        "    s=100,\n",
        ")\n",
        "plt.title(\"t-sne with colors corresponding to infomap communities\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRRcSKl3DOn1"
      },
      "source": [
        "db_dt = utils.dbscan_hyperparameters(\n",
        "    node_embeddings, e_lower=0.1, e_upper=0.9, m_lower=5, m_upper=15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jkwn2iGDSqA"
      },
      "source": [
        "db_dt.sort_values(by=[\"n_noise\"])[db_dt.n_clusters > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npJT-FwyDVqO"
      },
      "source": [
        "db = DBSCAN(eps=0.1, min_samples=5).fit(node_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYT6KFzkDYl9"
      },
      "source": [
        "labels = db.labels_\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
        "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(node_embeddings, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwnp1ffBDa05"
      },
      "source": [
        "emb_transformed[\"dbacan_clusters\"] = labels\n",
        "X = emb_transformed[emb_transformed[\"dbacan_clusters\"] != -1]\n",
        "\n",
        "\n",
        "plt.scatter(\n",
        "    X[0],\n",
        "    X[1],\n",
        "    c=X[\"dbacan_clusters\"],\n",
        "    cmap=\"Spectral\",\n",
        "    edgecolors=\"black\",\n",
        "    alpha=0.3,\n",
        "    s=100,\n",
        ")\n",
        "plt.title(\"t-sne with colors corresponding to dbscan cluster. Without noise points\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}