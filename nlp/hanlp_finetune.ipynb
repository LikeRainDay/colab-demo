{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Jdez6XNAFmTuLkJjnXcf72K72_G3IKe9",
      "authorship_tag": "ABX9TyNRL+Dru+j9LrFpYMFu/9k1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LikeRainDay/colab-demo/blob/main/nlp/hanlp_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmhVHvn7lQRo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hanlp\n",
        "!pip uninstall transformers -y\n",
        "!pip install transformers==4.41.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6rOaZLPlVDe",
        "outputId": "86fa1e43-b471-49b9-90a2-0e271532ca8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hanlp in /usr/local/lib/python3.11/dist-packages (2.1.1)\n",
            "Requirement already satisfied: hanlp-common>=0.0.23 in /usr/local/lib/python3.11/dist-packages (from hanlp) (0.0.23)\n",
            "Requirement already satisfied: hanlp-downloader in /usr/local/lib/python3.11/dist-packages (from hanlp) (0.0.25)\n",
            "Requirement already satisfied: hanlp-trie>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from hanlp) (0.0.5)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from hanlp) (12.0.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from hanlp) (0.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from hanlp) (3.1.0)\n",
            "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.11/dist-packages (from hanlp) (1.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from hanlp) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from hanlp) (4.28.1)\n",
            "Requirement already satisfied: phrasetree>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from hanlp-common>=0.0.23->hanlp) (0.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->hanlp) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->hanlp) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1.1->hanlp) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->hanlp) (12.575.51)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.1.1->hanlp) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->hanlp) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.1.1->hanlp) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.1.1->hanlp) (2025.7.9)\n",
            "Found existing installation: transformers 4.28.1\n",
            "Uninstalling transformers-4.28.1:\n",
            "  Successfully uninstalled transformers-4.28.1\n",
            "Collecting transformers==4.41.0\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.7.9)\n",
            "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.41.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "# Author: hankcs\n",
        "# Date: 2023-10-18 18:49\n",
        "import os\n",
        "\n",
        "import hanlp\n",
        "import torch\n",
        "from hanlp.components.ner.transformer_ner import TransformerNamedEntityRecognizer\n",
        "\n",
        "\n",
        "your_training_corpus = 'data/ner/finetune/word_to_iobes_tp.tsv'\n",
        "your_development_corpus = your_training_corpus  # Use a different one in reality\n",
        "save_dir = 'data/ner/finetune/model'\n",
        "\n",
        "if not os.path.exists(your_training_corpus):\n",
        "    os.makedirs(os.path.dirname(your_training_corpus), exist_ok=True)\n",
        "    with open(your_training_corpus, 'w') as out:\n",
        "        out.write(\n",
        "'''训练\\tB-LAW\n",
        "语料\\tE-LAW\n",
        "为\\tO\n",
        "IOBES\\tO\n",
        "格式\\tO\n",
        "'''\n",
        "        )\n",
        "\n",
        "ner = TransformerNamedEntityRecognizer()\n",
        "ner.fit(\n",
        "    trn_data=your_training_corpus,\n",
        "    dev_data=your_development_corpus,\n",
        "    save_dir=save_dir,\n",
        "    epochs=50,  # Since the corpus is small, overfit it\n",
        "    finetune=hanlp.pretrained.ner.MSRA_NER_ELECTRA_SMALL_ZH,\n",
        "    # You MUST set the same parameters with the fine-tuning model:\n",
        "    average_subwords=True,\n",
        "    transformer='hfl/chinese-electra-180g-small-discriminator',\n",
        ")\n",
        "\n",
        "device = 'cpu'\n",
        "HanLP = hanlp.pipeline()\\\n",
        "    .append(hanlp.load(hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH), output_key='tok')\\\n",
        "    .append(hanlp.load(save_dir), output_key='ner') # Load the trained model\n",
        "\n",
        "HanLP(['训练语料为IOBES格式', '晓美焰来到北京立方庭参观自然语义科技公司。']).pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9art0jYAlWN1",
        "outputId": "8460af1e-b7a9-4d7f-e495-ac559f882aa3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"adam_epsilon\": 1e-08,\n",
            "  \"average_subwords\": true,\n",
            "  \"batch_max_tokens\": null,\n",
            "  \"batch_size\": 32,\n",
            "  \"char_level\": false,\n",
            "  \"classpath\": \"hanlp.components.ner.transformer_ner.TransformerNamedEntityRecognizer\",\n",
            "  \"crf\": false,\n",
            "  \"delimiter_in_entity\": null,\n",
            "  \"epochs\": 50,\n",
            "  \"extra_embeddings\": null,\n",
            "  \"finetune\": \"https://file.hankcs.com/hanlp/ner/msra_ner_electra_small_20220215_205503.zip\",\n",
            "  \"grad_norm\": 5.0,\n",
            "  \"gradient_accumulation\": 1,\n",
            "  \"hanlp_version\": \"2.1.1\",\n",
            "  \"hard_constraint\": false,\n",
            "  \"hidden_dropout\": null,\n",
            "  \"layer_dropout\": 0,\n",
            "  \"lr\": 5e-05,\n",
            "  \"max_seq_len\": null,\n",
            "  \"merge_types\": null,\n",
            "  \"mix_embedding\": 0,\n",
            "  \"patience\": 5,\n",
            "  \"reduction\": \"sum\",\n",
            "  \"ret_raw_hidden_states\": false,\n",
            "  \"sampler_builder\": null,\n",
            "  \"scalar_mix\": null,\n",
            "  \"secondary_encoder\": null,\n",
            "  \"seed\": 1752676234,\n",
            "  \"sent_delimiter\": null,\n",
            "  \"tagset\": null,\n",
            "  \"token_key\": \"token\",\n",
            "  \"transform\": null,\n",
            "  \"transformer\": \"hfl/chinese-electra-180g-small-discriminator\",\n",
            "  \"transformer_grad_norm\": null,\n",
            "  \"transformer_layers\": null,\n",
            "  \"transformer_lr\": null,\n",
            "  \"warmup_steps\": 0.1,\n",
            "  \"weight_decay\": 0,\n",
            "  \"word_dropout\": 0.2\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finetune model loaded with 12686113/12686113 trainable/total parameters.\n",
            "tag[35] = ['O', 'S-INTEGER', 'S-ORDINAL', 'S-LOCATION', 'S-DATE', 'S-ORGANIZATION', 'B-LOCATION', 'M-LOCATION', 'E-LOCATION', 'S-PERSON', 'S-MONEY', 'S-DURATION', 'B-ORGANIZATION', 'E-ORGANIZATION', 'S-TIME', 'M-ORGANIZATION', 'S-LENGTH', 'S-AGE', 'S-FREQUENCY', 'S-ANGLE', 'S-PHONE', 'S-PERCENT', 'S-FRACTION', 'S-WEIGHT', 'S-AREA', 'S-CAPACTITY', 'S-DECIMAL', 'S-MEASURE', 'S-SPEED', 'S-TEMPERATURE', 'S-POSTALCODE', 'S-RATE', 'S-WWW', 'B-LAW', 'E-LAW']\n",
            "Pruned \u001b[33m0 (0.0%)\u001b[0m samples out of 1.\n",
            "Pruned \u001b[33m0 (0.0%)\u001b[0m samples out of 1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33mBuilding model \u001b[5m...\u001b[0m\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following parameters were missing from the checkpoint: classifier.bias, classifier.weight.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r                                   \r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built with 12686627/12686627 trainable/total parameters.\n",
            "Using \u001b[31mCPU\u001b[0m\n",
            "1/1 samples in trn/dev set.\n",
            "\u001b[33mEpoch 1 / 50:\u001b[0m\n",
            "1/1 loss: 5.4653 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 5.7827 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "0 s / 14 s ETA: 14 s \u001b[31m(saved)\u001b[0m\n",
            "\u001b[33mEpoch 2 / 50:\u001b[0m\n",
            "1/1 loss: 5.6755 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 5.4583 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1 s / 17 s ETA: 16 s (1)\n",
            "\u001b[33mEpoch 3 / 50:\u001b[0m\n",
            "1/1 loss: 5.1423 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 4.4069 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1 s / 15 s ETA: 15 s (2)\n",
            "\u001b[33mEpoch 4 / 50:\u001b[0m\n",
            "1/1 loss: 4.1301 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 2.8536 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1 s / 15 s ETA: 14 s (3)\n",
            "\u001b[33mEpoch 5 / 50:\u001b[0m\n",
            "1/1 loss: 2.5515 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 4.1417 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1 s / 14 s ETA: 13 s (4)\n",
            "\u001b[33mEpoch 6 / 50:\u001b[0m\n",
            "1/1 loss: 2.2077 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 4.7251 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "2 s / 14 s ETA: 13 s (5)\n",
            "\u001b[33mEpoch 7 / 50:\u001b[0m\n",
            "1/1 loss: 3.0124 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 3.9988 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "2 s / 14 s ETA: 12 s (6)\n",
            "\u001b[33mEpoch 8 / 50:\u001b[0m\n",
            "1/1 loss: 3.3565 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 1.8966 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "2 s / 14 s ETA: 12 s (7)\n",
            "\u001b[33mEpoch 9 / 50:\u001b[0m\n",
            "1/1 loss: 1.9752 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 1.0377 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "2 s / 13 s ETA: 11 s (8)\n",
            "\u001b[33mEpoch 10 / 50:\u001b[0m\n",
            "1/1 loss: 1.2579 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.9016 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "3 s / 13 s ETA: 11 s (9)\n",
            "\u001b[33mEpoch 11 / 50:\u001b[0m\n",
            "1/1 loss: 0.9078 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.8165 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "3 s / 13 s ETA: 10 s (10)\n",
            "\u001b[33mEpoch 12 / 50:\u001b[0m\n",
            "1/1 loss: 1.0271 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.7264 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "3 s / 13 s ETA: 10 s (11)\n",
            "\u001b[33mEpoch 13 / 50:\u001b[0m\n",
            "1/1 loss: 0.8776 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.6377 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "3 s / 13 s ETA: 9 s (12)\n",
            "\u001b[33mEpoch 14 / 50:\u001b[0m\n",
            "1/1 loss: 1.0148 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.6369 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "4 s / 13 s ETA: 9 s (13)\n",
            "\u001b[33mEpoch 15 / 50:\u001b[0m\n",
            "1/1 loss: 0.7377 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.6024 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "4 s / 13 s ETA: 9 s (14)\n",
            "\u001b[33mEpoch 16 / 50:\u001b[0m\n",
            "1/1 loss: 0.6584 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.5328 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "4 s / 13 s ETA: 9 s (15)\n",
            "\u001b[33mEpoch 17 / 50:\u001b[0m\n",
            "1/1 loss: 0.6810 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.4707 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "4 s / 13 s ETA: 8 s \u001b[31m(saved)\u001b[0m\n",
            "\u001b[33mEpoch 18 / 50:\u001b[0m\n",
            "1/1 loss: 0.6016 P: 0.00% R: 0.00% F1: 0.00% ET: 0 s\n",
            "1/1 loss: 0.4279 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "5 s / 13 s ETA: 9 s (1)\n",
            "\u001b[33mEpoch 19 / 50:\u001b[0m\n",
            "1/1 loss: 0.4650 P: 10.00% R: 5.26% F1: 6.90% ET: 0 s\n",
            "1/1 loss: 0.4020 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "5 s / 13 s ETA: 8 s (2)\n",
            "\u001b[33mEpoch 20 / 50:\u001b[0m\n",
            "1/1 loss: 0.6005 P: 10.00% R: 5.00% F1: 6.67% ET: 0 s\n",
            "1/1 loss: 0.3798 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "5 s / 14 s ETA: 8 s (3)\n",
            "\u001b[33mEpoch 21 / 50:\u001b[0m\n",
            "1/1 loss: 0.4370 P: 18.18% R: 9.52% F1: 12.50% ET: 0 s\n",
            "1/1 loss: 0.3648 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "6 s / 14 s ETA: 8 s (4)\n",
            "\u001b[33mEpoch 22 / 50:\u001b[0m\n",
            "1/1 loss: 0.4666 P: 25.00% R: 13.64% F1: 17.65% ET: 0 s\n",
            "1/1 loss: 0.3294 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "6 s / 14 s ETA: 8 s (5)\n",
            "\u001b[33mEpoch 23 / 50:\u001b[0m\n",
            "1/1 loss: 0.7746 P: 21.43% R: 13.04% F1: 16.22% ET: 0 s\n",
            "1/1 loss: 0.3037 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "6 s / 14 s ETA: 7 s (6)\n",
            "\u001b[33mEpoch 24 / 50:\u001b[0m\n",
            "1/1 loss: 0.2698 P: 26.67% R: 16.67% F1: 20.51% ET: 0 s\n",
            "1/1 loss: 0.2821 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "7 s / 14 s ETA: 7 s (7)\n",
            "\u001b[33mEpoch 25 / 50:\u001b[0m\n",
            "1/1 loss: 0.4380 P: 25.00% R: 16.00% F1: 19.51% ET: 0 s\n",
            "1/1 loss: 0.2598 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "7 s / 14 s ETA: 7 s (8)\n",
            "\u001b[33mEpoch 26 / 50:\u001b[0m\n",
            "1/1 loss: 0.2489 P: 29.41% R: 19.23% F1: 23.26% ET: 0 s\n",
            "1/1 loss: 0.2459 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "7 s / 14 s ETA: 7 s (9)\n",
            "\u001b[33mEpoch 27 / 50:\u001b[0m\n",
            "1/1 loss: 0.2639 P: 33.33% R: 22.22% F1: 26.67% ET: 0 s\n",
            "1/1 loss: 0.2328 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "8 s / 14 s ETA: 7 s (10)\n",
            "\u001b[33mEpoch 28 / 50:\u001b[0m\n",
            "1/1 loss: 0.4132 P: 31.58% R: 21.43% F1: 25.53% ET: 0 s\n",
            "1/1 loss: 0.2237 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "8 s / 14 s ETA: 6 s (11)\n",
            "\u001b[33mEpoch 29 / 50:\u001b[0m\n",
            "1/1 loss: 0.2127 P: 35.00% R: 24.14% F1: 28.57% ET: 0 s\n",
            "1/1 loss: 0.2170 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "8 s / 14 s ETA: 6 s (12)\n",
            "\u001b[33mEpoch 30 / 50:\u001b[0m\n",
            "1/1 loss: 0.3006 P: 38.10% R: 26.67% F1: 31.37% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "9 s / 15 s ETA: 6 s (13)\n",
            "\u001b[33mEpoch 31 / 50:\u001b[0m\n",
            "1/1 loss: 0.5630 P: 40.91% R: 29.03% F1: 33.96% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "9 s / 15 s ETA: 6 s (14)\n",
            "\u001b[33mEpoch 32 / 50:\u001b[0m\n",
            "1/1 loss: 0.3792 P: 43.48% R: 31.25% F1: 36.36% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "9 s / 14 s ETA: 5 s (15)\n",
            "\u001b[33mEpoch 33 / 50:\u001b[0m\n",
            "1/1 loss: 0.3500 P: 45.83% R: 33.33% F1: 38.60% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "9 s / 14 s ETA: 5 s (16)\n",
            "\u001b[33mEpoch 34 / 50:\u001b[0m\n",
            "1/1 loss: 0.2549 P: 48.00% R: 35.29% F1: 40.68% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "10 s / 14 s ETA: 5 s (17)\n",
            "\u001b[33mEpoch 35 / 50:\u001b[0m\n",
            "1/1 loss: 0.4033 P: 50.00% R: 37.14% F1: 42.62% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "10 s / 14 s ETA: 4 s (18)\n",
            "\u001b[33mEpoch 36 / 50:\u001b[0m\n",
            "1/1 loss: 0.3100 P: 51.85% R: 38.89% F1: 44.44% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "10 s / 14 s ETA: 4 s (19)\n",
            "\u001b[33mEpoch 37 / 50:\u001b[0m\n",
            "1/1 loss: 0.2158 P: 53.57% R: 40.54% F1: 46.15% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "10 s / 14 s ETA: 4 s (20)\n",
            "\u001b[33mEpoch 38 / 50:\u001b[0m\n",
            "1/1 loss: 0.3161 P: 55.17% R: 42.11% F1: 47.76% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "11 s / 14 s ETA: 3 s (21)\n",
            "\u001b[33mEpoch 39 / 50:\u001b[0m\n",
            "1/1 loss: 0.3217 P: 56.67% R: 43.59% F1: 49.28% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "11 s / 14 s ETA: 3 s (22)\n",
            "\u001b[33mEpoch 40 / 50:\u001b[0m\n",
            "1/1 loss: 0.2553 P: 58.06% R: 45.00% F1: 50.70% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "11 s / 14 s ETA: 3 s (23)\n",
            "\u001b[33mEpoch 41 / 50:\u001b[0m\n",
            "1/1 loss: 0.4818 P: 56.25% R: 43.90% F1: 49.32% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "11 s / 14 s ETA: 2 s (24)\n",
            "\u001b[33mEpoch 42 / 50:\u001b[0m\n",
            "1/1 loss: 0.2517 P: 57.58% R: 45.24% F1: 50.67% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "12 s / 14 s ETA: 2 s (25)\n",
            "\u001b[33mEpoch 43 / 50:\u001b[0m\n",
            "1/1 loss: 0.2967 P: 58.82% R: 46.51% F1: 51.95% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "12 s / 14 s ETA: 2 s (26)\n",
            "\u001b[33mEpoch 44 / 50:\u001b[0m\n",
            "1/1 loss: 0.3404 P: 60.00% R: 47.73% F1: 53.16% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "12 s / 14 s ETA: 2 s (27)\n",
            "\u001b[33mEpoch 45 / 50:\u001b[0m\n",
            "1/1 loss: 0.4596 P: 56.76% R: 46.67% F1: 51.22% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "12 s / 14 s ETA: 1 s (28)\n",
            "\u001b[33mEpoch 46 / 50:\u001b[0m\n",
            "1/1 loss: 0.3390 P: 57.89% R: 47.83% F1: 52.38% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "12 s / 14 s ETA: 1 s (29)\n",
            "\u001b[33mEpoch 47 / 50:\u001b[0m\n",
            "1/1 loss: 0.2773 P: 58.97% R: 48.94% F1: 53.49% ET: 0 s\n",
            "1/1 loss: 0.2137 P: 100.00% R: 100.00% F1: 100.00% ET: 0 s\n",
            "13 s / 13 s ETA: 1 s (30) early stop\n",
            "Max score of dev is P: 100.00% R: 100.00% F1: 100.00% at epoch 17\n",
            "Average time of each epoch is 0 s\n",
            "13 s elapsed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Token&nbsp;<br>─────&nbsp;<br>训练&nbsp;&nbsp;&nbsp;&nbsp;<br>语料&nbsp;&nbsp;&nbsp;&nbsp;<br>为&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>IOBES&nbsp;<br>格式&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">NER&nbsp;Typ<br>───────<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┴►LAW<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre></div><br><div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>晓美焰&nbsp;<br>来到&nbsp;&nbsp;<br>北京&nbsp;&nbsp;<br>立方庭&nbsp;<br>参观&nbsp;&nbsp;<br>自然&nbsp;&nbsp;<br>语义&nbsp;&nbsp;<br>科技&nbsp;&nbsp;<br>公司&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">NER&nbsp;Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>────────────────<br>───►PERSON&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>───►LOCATION&nbsp;&nbsp;&nbsp;&nbsp;<br>───►LOCATION&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;├►ORGANIZATION<br>◄─┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}